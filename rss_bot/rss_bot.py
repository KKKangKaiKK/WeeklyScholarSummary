import requests
import json
import os
import re
import time
from datetime import datetime, timedelta,date
from concurrent.futures import ThreadPoolExecutor
import feedparser
from bs4 import BeautifulSoup
import html



# --- æ–°å¢ï¼šæŠ‘åˆ¶é’ˆå¯¹ç‰¹å®šè¿æ¥çš„ InsecureRequestWarning ---
# æˆ‘ä»¬å°†åœ¨ä»£ç ä¸­ç²¾ç¡®æ§åˆ¶ SSL éªŒè¯ï¼Œå› æ­¤å¯ä»¥å®‰å…¨åœ°ç¦ç”¨æ­¤è­¦å‘Š
import urllib3
from urllib3.exceptions import InsecureRequestWarning
urllib3.disable_warnings(InsecureRequestWarning)
# ----------------------------------------------------

# --- 1. é…ç½®ä¸ç¼“å­˜ç®¡ç† ---

def load_config(filename="config.json"):
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šé…ç½®æ–‡ä»¶ '{filename}' æœªæ‰¾åˆ°ã€‚è¯·æ ¹æ®æ¨¡æ¿åˆ›å»ºã€‚")
        exit()

def save_cache(data, filename):
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    print(f"è¿›åº¦å·²ä¿å­˜åˆ°: {filename}")

def load_cache(filename):
    if os.path.exists(filename):
        with open(filename, 'r', encoding='utf-8') as f:
            print(f"ä»ç¼“å­˜åŠ è½½: {filename}")
            return json.load(f)
    return None

# --- 2. åŸºäº Requests çš„ LLM å®¢æˆ·ç«¯ ---

def requests_based_llm_call(client_config, prompt):
    """
    ä½¿ç”¨ requests åº“è°ƒç”¨ APIï¼Œæ”¯æŒè¶…æ—¶è‡ªåŠ¨é‡è¯•ã€‚
    """
    api_base = client_config['api_base']
    api_key = client_config['api_key']
    model = client_config['model']
    verify_ssl = client_config.get('verify_ssl', True)
    
    # è·å–æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä¸º 3 æ¬¡ (å³å¦‚æœè¶…æ—¶ï¼Œä¼šé¢å¤–å°è¯• 2 æ¬¡)
    max_retries = client_config.get('max_retries', 3)
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    
    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.7,
    }

    # å¼€å§‹å¾ªç¯å°è¯•
    for attempt in range(1, max_retries + 1):
        try:
            # æ‰“å°è°ƒè¯•ä¿¡æ¯ (å¯é€‰)
            # if attempt > 1:
            #     print(f"æ­£åœ¨è¿›è¡Œç¬¬ {attempt} æ¬¡å°è¯•...")

            response = requests.post(
                f"{api_base}/chat/completions",
                headers=headers,
                json=payload,
                verify=verify_ssl,
                timeout=240 # è®¾ç½®è¶…æ—¶æ—¶é—´
            )
            response.raise_for_status()
            
            # --- è§£ææˆåŠŸï¼Œå¤„ç†æ•°æ® ---
            data = response.json()
            content = data['choices'][0]['message']['content']
            cleaned_content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)
            return cleaned_content.strip()

        except requests.exceptions.Timeout as e:
            # ä¸“é—¨æ•è·è¶…æ—¶é”™è¯¯ (Read timed out / Connect timed out)
            print(f"[{client_config['name']}] ç¬¬ {attempt} æ¬¡è¯·æ±‚è¶…æ—¶: {e}")
            
            if attempt < max_retries:
                wait_time = 2  # é‡è¯•å‰ç­‰å¾… 2 ç§’
                print(f"å°†åœ¨ {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
                continue  # è¿›å…¥ä¸‹ä¸€æ¬¡å¾ªç¯
            else:
                print(f"[{client_config['name']}] å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒã€‚")
                return None

        except requests.exceptions.RequestException as e:
            # æ•è·å…¶ä»–ç½‘ç»œé”™è¯¯ (å¦‚ 404, 500, DNS é”™è¯¯ç­‰)ï¼Œé€šå¸¸è¿™äº›ä¸éœ€è¦ç«‹å³é‡è¯•æˆ–éœ€å•ç‹¬å¤„ç†
            print(f"ä¸ LLM æœåŠ¡å™¨ ({client_config['name']}) é€šä¿¡æ—¶å‘ç”Ÿéè¶…æ—¶ç½‘ç»œé”™è¯¯: {e}")
            return None
            
        except (KeyError, IndexError) as e:
            # è§£æé”™è¯¯ï¼Œè¯´æ˜è¿æ¥æˆåŠŸä½†è¿”å›æ ¼å¼ä¸å¯¹ï¼Œä¸éœ€è¦é‡è¯•
            print(f"è§£æ LLM æœåŠ¡å™¨ ({client_config['name']}) çš„å“åº”æ—¶å‡ºé”™: {e}")
            # ä¸ºäº†è°ƒè¯•ï¼Œå¯ä»¥æ‰“å° response.textï¼Œä½†åœ¨å‘ç”Ÿå¼‚å¸¸æ—¶ response å¯èƒ½æœªå®šä¹‰ï¼Œéœ€å°å¿ƒ
            return None

    return None
# --- 3. RSS æ‹‰å–ä¸å†…å®¹å¤„ç† ---

def get_article_full_content(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}
        # ä¿®æ”¹ï¼šç§»é™¤ verify=Falseï¼Œæ¢å¤å¯¹å¤–éƒ¨ç½‘ç«™çš„è¯ä¹¦éªŒè¯
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        article_body = soup.find('article') or soup.find('div', class_='post-content') or soup.find('div', class_='content')
        if article_body:
            return article_body.get_text(separator='\n', strip=True)
        return None
    except Exception:
        return None

def fetch_and_filter_rss(feed_urls, days_to_scan, existing_links):
    scan_since_date = datetime.now() - timedelta(days=days_to_scan)
    new_articles = []

    for url in feed_urls:
        print(f"æ­£åœ¨å¤„ç† RSSæº: {url}")
        feed = feedparser.parse(url)
        for entry in feed.entries:
            published_time = None
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                published_time = datetime.fromtimestamp(time.mktime(entry.published_parsed))
            
            if published_time and published_time >= scan_since_date:
                if entry.link in existing_links:
                    continue

                content = entry.get('summary', '')
                full_content = get_article_full_content(entry.link)
                if full_content:
                    content = full_content

                new_articles.append({
                    "title": entry.title,
                    "link": entry.link,
                    "content": content,
                    "published": published_time.strftime("%Y-%m-%d"),
                    "category": None
                })
    return new_articles

# --- 4. LLM æ ¸å¿ƒé€»è¾‘ (åˆ†ç±»ä¸æ€»ç»“) ---

def classify_article_worker(article_with_index, client_config, topics):
    index, article = article_with_index
    print(f"  [çº¿ç¨‹: {client_config['name']}] æ­£åœ¨åˆ†ç±»: {article['title'][:30]}...")
    
    prompt = f"""
    è¯·åˆ¤æ–­ä»¥ä¸‹æ–‡ç« å†…å®¹ä¸»è¦ä¸å“ªä¸ªè¯é¢˜æœ€ç›¸å…³ã€‚è¯é¢˜åˆ—è¡¨ï¼š{topics}ã€‚
    åˆ†ç±»åŸåˆ™ï¼ˆéå¸¸é‡è¦ï¼‰ï¼š
    1. **ä¼˜å…ˆç‰¹å¼‚æ€§**ï¼šå¦‚æœæ–‡ç« å†…å®¹åŒæ—¶ç¬¦åˆâ€œè¯é¢˜Aâ€å’Œâ€œè¯é¢˜Açš„å­é¢†åŸŸBâ€ï¼Œå¿…é¡»è¿”å› **â€œè¯é¢˜Bâ€**ã€‚
    2. **æ‹’ç»å®½æ³›**ï¼šåªæœ‰åœ¨æ–‡ç« å†…å®¹å®Œå…¨æ— æ³•åŒ¹é…ä»»ä½•æ›´å…·ä½“çš„å­è¯é¢˜æ—¶ï¼Œæ‰å…è®¸è¿”å›è¾ƒå®½æ³›çš„çˆ¶çº§è¯é¢˜ã€‚
    3. **ç²¾å‡†åŒ¹é…**ï¼šè¯·è¿”å›è¯é¢˜åˆ—è¡¨ä¸­æœ€ç‹­çª„ã€å®šä¹‰æœ€å…·ä½“çš„é‚£ä¸ªæ¦‚å¿µã€‚
    4. å¦‚æœå†…å®¹ä¸ä»»ä½•ä¸€ä¸ªè¯é¢˜éƒ½ä¸å¤ªç›¸å…³ï¼Œè¯·è¿”å› "å…¶ä»–"ã€‚

    å¦‚æœä½ æ”¯æŒæ¨ç†ï¼Œè¯·ç®€çŸ­æ€è€ƒè¯¥æ–‡ç« æ˜¯å¦å±äºæŸä¸ªè¯é¢˜çš„ç»†åˆ†é¢†åŸŸï¼›å¦‚æœä¸æ”¯æŒï¼Œè¯·ç›´æ¥è¾“å‡ºç»“æœã€‚
    è¯·åªè¿”å›æœ€ç›¸å…³çš„è¯é¢˜åç§°ï¼Œ**ä¸è¦**æ·»åŠ ä»»ä½•è§£é‡Šã€æ ‡ç‚¹ç¬¦å·æˆ–å‰ç¼€åç¼€ã€‚

    æ–‡ç« å†…å®¹ï¼š
    ---
    {article['title']}
    {article['content'][:500]}
    ---
    æœ€ç›¸å…³çš„è¯é¢˜æ˜¯ï¼š
    """
    category = requests_based_llm_call(client_config, prompt)
    print(f"  {article['title'][:40]}...ï¼Œåˆ†ç±»ç»“æœï¼š{category}")
    return index, category


def summarize_articles(client_config, articles_content):
    # --- MODIFICATION START (Request 3: AI instruction) ---
    # æ·»åŠ äº†æ˜ç¡®æŒ‡ä»¤ï¼Œè¦æ±‚AIä¸è¦ä½¿ç”¨MarkdownåŠ ç²—
    prompt = f"""
    è¯·æ ¹æ®ä»¥ä¸‹æ–‡ç« å†…å®¹ï¼Œä¸ºè¿™ä¸ªä¸»é¢˜ç”Ÿæˆä¸€ä¸ªä¸°å¯Œã€æœ‰æ¡ç†çš„å‘¨æŠ¥æ€»ç»“ã€‚æ²¡æœ‰ç¯‡å¹…é™åˆ¶ï¼Œæ€»ç»“è¶Šé•¿è¶Šå¥½ã€‚
    è¯·ç›´æ¥å¼€å§‹å†™æ€»ç»“ï¼Œä¸è¦æœ‰å¼•è¨€ã€‚
    é‡è¦ï¼šåœ¨ä½ çš„å›ç­”ä¸­ï¼Œè¯·ä¸è¦ä½¿ç”¨ Markdown çš„åŠ ç²—è¯­æ³•ï¼ˆä¾‹å¦‚ **æ–‡å­—**ï¼‰ï¼Œç›´æ¥ç”Ÿæˆçº¯æ–‡æœ¬å³å¯ã€‚

    æ–‡ç« å†…å®¹ï¼š
    ---
    {articles_content}
    ---
    æœ¬å‘¨æ€»ç»“ï¼š
    """
    # --- MODIFICATION END ---
    return requests_based_llm_call(client_config, prompt)


# --- 5. HTML ç”Ÿæˆ (å†…è” CSS) ---

# --- MODIFICATION START (Request 1, 2, 3: HTML generation) ---
# å…¨é¢é‡æ„æ­¤å‡½æ•°ä»¥å®ç°æ–°çš„è®¾è®¡ã€å¤„ç†ç©ºè¯é¢˜å¹¶æ·»åŠ å†å²é“¾æ¥
def generate_html_inline_css(categorized_articles, summaries, filename):
    report_date = datetime.now().strftime('%Y-%m-%d')
    
    # æ›´ç°ä»£åŒ–çš„æ ·å¼è®¾è®¡
    styles = {
        'body': 'font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Helvetica Neue", "Arial", "PingFang SC", "Microsoft YaHei", sans-serif; line-height: 1.7; color: #4a4a4a; background-color: #f4f7f9; margin: 0; padding: 15px;',
        'container': 'max-width: 800px; margin: 20px auto; background-color: #ffffff; padding: 30px 40px; border-radius: 12px; box-shadow: 0 8px 30px rgba(0,0,0,0.07);',
        'header': 'text-align: center; border-bottom: 1px solid #e0e0e0; padding-bottom: 20px; margin-bottom: 35px;',
        'h1': 'margin: 0; color: #2c3e50; font-size: 28px; font-weight: 700;',
        'date': 'color: #888; font-size: 15px; margin-top: 8px;',
        'history_link_p': 'margin-top: 15px;',
        'history_link_a': 'color: #3498db; text-decoration: none; font-size: 14px; transition: color 0.3s;',
        'category_section': 'margin-bottom: 40px;',
        'h2': 'color: #2980b9; border-bottom: 2px solid #3498db; padding-bottom: 10px; margin-top: 0; font-size: 24px; font-weight: 600;',
        'summary_box': 'background-color: #f8f9fa; padding: 20px 25px; border-radius: 8px; margin-bottom: 28px; border-left: 4px solid #3498db;',
        'h3': 'margin-top: 0; margin-bottom: 12px; color: #34495e; font-size: 18px; font-weight: 600;',
        'p': 'margin: 0; line-height: 1.8;',
        'h4': 'margin-top: 0; margin-bottom: 18px; font-size: 17px; color: #34495e; font-weight: 600;',
        'ul': 'list-style-type: none; padding: 0; margin: 0;',
        'li': 'margin-bottom: 16px; padding-left: 22px; position: relative;',
        'li_before': 'content: "â–ª"; color: #3498db; position: absolute; left: 0; top: 1px; font-size: 18px;',
        'a': 'text-decoration: none; color: #2c3e50; font-weight: 500; font-size: 16px; transition: color 0.3s;',
        'meta': 'color: #7f8c8d; font-size: 13px; margin-left: 10px;',
        'footer': 'text-align: center; margin-top: 40px; padding-top: 25px; border-top: 1px solid #e0e0e0; color: #b0b0b0; font-size: 13px;',
        'no_articles_box': 'background-color: #fcfcfc; color: #999; padding: 20px 25px; border-radius: 8px; border: 1px dashed #ddd; text-align: center; font-style: italic;'
    }
    
    html_content = f"""
    <!DOCTYPE html>
    <html lang="zh-CN">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>æ¯å‘¨RSSåŠ¨æ€æ€»ç»“ - {report_date}</title>
        <style>
            a:hover {{ color: #e74c3c !important; }}
        </style>
    </head>
    <body style="{styles['body']}">
        <div style="{styles['container']}">
            <div style="{styles['header']}">
                <h1 style="{styles['h1']}">æ¯å‘¨ RSS åŠ¨æ€æ€»ç»“</h1>
                <p style="{styles['date']}">{report_date}</p>
                {'''<!-- Request 1: æ·»åŠ å›é¡¾å†å²æ±‡æ€»é“¾æ¥ -->'''}
                <p style="{styles['history_link_p']}">
                    <a href="https://github.com/KKKangKaiKK/WeeklyScholarSummary/tree/main/summaries" target="_blank" rel="noopener noreferrer" style="{styles['history_link_a']}">ğŸ”— å›é¡¾å†å²æ±‡æ€»</a>
                </p>
            </div>
    """
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æ–‡ç« è¢«åˆ†ç±»
    if not categorized_articles:
        html_content += f"<div style='{styles['no_articles_box']}'><p>æœ¬å‘¨æ‰«æèŒƒå›´å†…ï¼Œæ²¡æœ‰å‘ç°ä¸æ‚¨é…ç½®çš„ä»»ä½•è¯é¢˜ç›¸å…³çš„æ–°æ–‡ç« ã€‚</p></div>"
    else:
        # Request 2: å¾ªç¯éå†æ‰€æœ‰æ„Ÿå…´è¶£çš„è¯é¢˜ï¼Œè€Œä¸ä»…ä»…æ˜¯æœ‰æ€»ç»“çš„è¯é¢˜
        for category, articles in categorized_articles.items():
            safe_category = html.escape(category)
            
            html_content += f"""
            <div style="{styles['category_section']}">
                <h2 style="{styles['h2']}">{safe_category}</h2>
            """
            
            # å¦‚æœè¿™ä¸ªåˆ†ç±»ä¸‹æœ‰æ–‡ç« 
            if articles:
                summary = summaries.get(category) # ä»summarieså­—å…¸è·å–æ€»ç»“
                if summary:
                    formatted_summary = html.escape(summary).replace('\n', '<br>')
                    html_content += f"""
                    <div style="{styles['summary_box']}">
                        <h3 style="{styles['h3']}">æœ¬å‘¨è§‚å¯Ÿ</h3>
                        <p style="{styles['p']}">{formatted_summary}</p>
                    </div>
                    """
                
                html_content += f"""
                <div>
                    <h4 style="{styles['h4']}">ç›¸å…³æ–‡ç« åˆ—è¡¨</h4>
                    <ul style="{styles['ul']}">
                """
                
                for article in articles:
                    safe_title = html.escape(article['title'])
                    safe_link = html.escape(article['link'])
                    safe_published = html.escape(article['published'])
                    # ä½¿ç”¨ <li> ä¼ªå…ƒç´  :before æ¥åˆ›å»ºé¡¹ç›®ç¬¦å·
                    html_content += f"""
                        <li style="{styles['li']}">
                            <span style='{styles['li_before']}'></span>
                            <a href="{safe_link}" target="_blank" rel="noopener noreferrer" style="{styles['a']}">{safe_title}</a>
                            <span style="{styles['meta']}">({safe_published})</span>
                        </li>
                    """
                html_content += "</ul></div>"
            
            # å¦‚æœè¿™ä¸ªåˆ†ç±»ä¸‹æ²¡æœ‰æ–‡ç« 
            else:
                html_content += f"""
                <div style="{styles['no_articles_box']}">
                    <p>æœ¬å‘¨æœªå‘ç°å…³äº â€œ{safe_category}â€ è¯é¢˜çš„æ–°æ–‡ç« ã€‚</p>
                </div>
                """
            
            html_content += "</div>"

    html_content += f"""
            <div style="{styles['footer']}">
                <p>ç”± RSS Summary Bot è‡ªåŠ¨ç”Ÿæˆ</p>
            </div>
        </div>
    </body>
    </html>
    """

    with open(filename, 'w', encoding='utf-8') as f:
        f.write(html_content)
    print(f"HTML æŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
# --- MODIFICATION END ---


# --- 6. ä¸»é€»è¾‘ ---

def main():
    config = load_config()
    prefix = config['cache_filename_prefix']
    
    last_week_file = f"{prefix}_last_week.json"
    fetched_file = f"{prefix}_fetched.json"
    classified_file = f"{prefix}_classified.json"

    last_week_data = load_cache(last_week_file)
    if last_week_data:
        days_to_scan = 8
        existing_links = {article['link'] for article in last_week_data}
        print(f"æ£€æµ‹åˆ°ä¸Šå‘¨è®°å½•ï¼Œå°†æ‰«æè¿‡å» {days_to_scan} å¤©çš„å†…å®¹ï¼Œå¹¶æ’é™¤ {len(existing_links)} ä¸ªæ—§é“¾æ¥ã€‚")
    else:
        days_to_scan = 15
        existing_links = set()
        print(f"æœªæ£€æµ‹åˆ°ä¸Šå‘¨è®°å½•ï¼Œå°†è¿›è¡Œé¦–æ¬¡æ‰«æï¼ŒèŒƒå›´ä¸ºè¿‡å» {days_to_scan} å¤©ã€‚")

    articles_to_process = load_cache(fetched_file)
    if not articles_to_process:
        print("\n--- å¼€å§‹è·å–æ–°æ–‡ç«  ---")
        articles_to_process = fetch_and_filter_rss(config['rss_feeds'], days_to_scan, existing_links)
        print(f"è·å–åˆ° {len(articles_to_process)} ç¯‡æ–°æ–‡ç« ã€‚")
        save_cache(articles_to_process, fetched_file)
    else:
        print(f"ä»ç¼“å­˜åŠ è½½äº† {len(articles_to_process)} ç¯‡å¾…å¤„ç†æ–‡ç« ã€‚")


    classified_articles = load_cache(classified_file)
    if not classified_articles:
        print("\n--- å¼€å§‹å¹¶è¡Œåˆ†ç±»æ–‡ç«  ---")
        clients = config['llm_clients'][1:]
        num_clients = len(clients)
        tasks = [( (i, article), clients[i % num_clients], config['interesting_topics'] ) for i, article in enumerate(articles_to_process)]
        
        with ThreadPoolExecutor(max_workers=num_clients) as executor:
            results = executor.map(lambda p: classify_article_worker(*p), tasks)

        for index, category in results:
            if category and category != "å…¶ä»–":
                articles_to_process[index]['category'] = category
        
        classified_articles = [a for a in articles_to_process if a['category']]
        print(f"åˆ†ç±»å®Œæˆï¼Œå…±æœ‰ {len(classified_articles)} ç¯‡ç›¸å…³æ–‡ç« ã€‚")
        save_cache(classified_articles, classified_file)
    else:
         print(f"ä»ç¼“å­˜åŠ è½½äº† {len(classified_articles)} ç¯‡å·²åˆ†ç±»æ–‡ç« ã€‚")

    print("\n--- å¼€å§‹ç”Ÿæˆæ€»ç»“ ---")
    categorized_articles = {}
    for topic in config['interesting_topics']:
        categorized_articles[topic] = [a for a in classified_articles if a['category'] == topic]

    summaries = {}
    summarizer_client = config['llm_clients'][0]
    for category, articles in categorized_articles.items():
        if not articles:
            continue
        print(f"æ­£åœ¨æ€»ç»“ç±»åˆ«: {category} ({len(articles)}ç¯‡æ–‡ç« )")
        combined_content = "\n\n---\n\n".join(
            [f"æ ‡é¢˜: {a['title']}\nå†…å®¹æ‘˜è¦: {a['content'][:1000]}" for a in articles]
        )
        summary = summarize_articles(summarizer_client, combined_content)
        if summary:
            summaries[category] = summary

    print("\n--- å¼€å§‹ç”Ÿæˆ HTML æŠ¥å‘Š ---")

    today = date.today()
    formatted_date = today.strftime("%Y-%m-%d")
    generate_html_inline_css(categorized_articles, summaries, f"{formatted_date}.html")

    print("\n--- æ¸…ç†ç¼“å­˜å¹¶æ›´æ–°çŠ¶æ€ ---")
    if os.path.exists(last_week_file):
        os.remove(last_week_file)
        print(f"å·²åˆ é™¤æ—§çš„çŠ¶æ€æ–‡ä»¶: {last_week_file}")
    
    if os.path.exists(classified_file):
        os.rename(classified_file, last_week_file)
        print(f"å½“å‰åˆ†ç±»ç»“æœå·²å­˜ä¸ºä¸‹ä¸€å‘¨çš„è®°å½•: {last_week_file}")

    if os.path.exists(fetched_file):
        os.remove(fetched_file)
        print(f"å·²åˆ é™¤æœ¬æ¬¡çš„ä¸­é—´ç¼“å­˜: {fetched_file}")

    print("\nä»»åŠ¡å…¨éƒ¨å®Œæˆï¼")


if __name__ == "__main__":
    main()
